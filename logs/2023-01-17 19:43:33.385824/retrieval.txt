Question 0 : hello. who are you?
Answer : 
content : Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. â€  Work performed while at Google Brain. â€¡ Work performed while at Google Research.  31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.transduction problems such as language modeling and machine translation [35, 2, 5]. 
score : 0.5281894225189722
content : Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. 
score : 0.5273901002193745
content : 1  Introduction  Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and âˆ— Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. 
score : 0.5265212822021862
content : The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. Decoder: The decoder is also composed of a stack of N = 6 identical layers. 
score : 0.5258695826018024
content : 6  Results  6.1  Machine Translation  On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. 
score : 0.5251005045386914


Question 1 : Can you explain about the self-attention?
Answer : 
content : Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. 
score : 0.5634868107354019
content : In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. 
score : 0.5597597122894644
content : Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. 
score : 0.558253835097703
content : 4  Why Self-Attention  In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1 , ..., xn ) to another sequence of equal length (z1 , ..., zn ), with xi , zi âˆˆ Rd , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. 
score : 0.5567987557795455
content : 4  Why Self-Attention  In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1 , ..., xn ) to another sequence of equal length (z1 , ..., zn ), with xi , zi âˆˆ Rd , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. 
score : 0.5550763468740101


Question 2 : OK. Thank you. you helped me a lot.
Answer : 
content : Model ByteNet [18] Deep-Att + PosUnk [39] GNMT + RL [38] ConvS2S [9] MoE [32] Deep-Att + PosUnk Ensemble [39] GNMT + RL Ensemble [38] ConvS2S Ensemble [9] Transformer (base model) Transformer (big)  BLEU EN-DE 23.75 24.6 25.16 26.03 26.30 26.36 27.3 28.4  EN-FR 39.2 39.92 40.46 40.56 40.4 41.16 41.29 38.1 41.8  Training Cost (FLOPs) EN-DE  EN-FR  1.0 Â· 1020 1.4 Â· 1020 1.5 Â· 1020 1.2 Â· 1020 8.0 Â· 1020 20 1.8 Â· 10 1.1 Â· 1021 19 7.7 Â· 10 1.2 Â· 1021 3.3 Â· 1018 2.3 Â· 1019 2.3 Â· 1019 9.6 Â· 1018 2.0 Â· 1019  Label Smoothing During training, we employed label smoothing of value ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.  
score : 0.5358673322418019
content : base  N  dmodel  dff  h  dk  dv  Pdrop  ls  6  512  2048  8 1 4 16 32  64 512 128 32 16 16 32  64 512 128 32 16  0.1  0.1  32 128  32 128  (A)  (B)  train steps 100K  2 4 8 (C)  256 1024 1024 4096  0.0 0.2  (D) (E) big  6  0.0 0.2 positional embedding instead of sinusoids 1024 4096 16 0.3  300K  PPL (dev) 4.92 5.29 5.00 4.91 5.01 5.16 5.01 6.11 5.19 4.88 5.75 4.66 5.12 4.75 5.77 4.95 4.67 5.47 4.92 4.33  BLEU (dev) 25.8 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4  params Ã—106 65  58 60 36 50 80 28 168 53 90  213  Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al. (2016) [8] WSJ only, discriminative 91.7 Transformer (4 layers) WSJ only, discriminative 91.3 Zhu et al. 
score : 0.53474011320464
content : base  N  dmodel  dff  h  dk  dv  Pdrop  ls  6  512  2048  8 1 4 16 32  64 512 128 32 16 16 32  64 512 128 32 16  0.1  0.1  32 128  32 128  (A)  (B)  train steps 100K  2 4 8 (C)  256 1024 1024 4096  0.0 0.2  (D) (E) big  6  0.0 0.2 positional embedding instead of sinusoids 1024 4096 16 0.3  300K  PPL (dev) 4.92 5.29 5.00 4.91 5.01 5.16 5.01 6.11 5.19 4.88 5.75 4.66 5.12 4.75 5.77 4.95 4.67 5.47 4.92 4.33  BLEU (dev) 25.8 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4  params Ã—106 65  58 60 36 50 80 28 168 53 90  213  Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. 
score : 0.5342805130843048
content : base  N  dmodel  dff  h  dk  dv  Pdrop  ls  6  512  2048  8 1 4 16 32  64 512 128 32 16 16 32  64 512 128 32 16  0.1  0.1  32 128  32 128  (A)  (B)  train steps 100K  2 4 8 (C)  256 1024 1024 4096  0.0 0.2  (D) (E) big  6  0.0 0.2 positional embedding instead of sinusoids 1024 4096 16 0.3  300K  PPL (dev) 4.92 5.29 5.00 4.91 5.01 5.16 5.01 6.11 5.19 4.88 5.75 4.66 5.12 4.75 5.77 4.95 4.67 5.47 4.92 4.33  BLEU (dev) 25.8 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4  params Ã—106 65  58 60 36 50 80 28 168 53 90  213  Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative 90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al. 
score : 0.5342788614907864
content : base  N  dmodel  dff  h  dk  dv  Pdrop  ls  6  512  2048  8 1 4 16 32  64 512 128 32 16 16 32  64 512 128 32 16  0.1  0.1  32 128  32 128  (A)  (B)  train steps 100K  2 4 8 (C)  256 1024 1024 4096  0.0 0.2  (D) (E) big  6  0.0 0.2 positional embedding instead of sinusoids 1024 4096 16 0.3  300K  PPL (dev) 4.92 5.29 5.00 4.91 5.01 5.16 5.01 6.11 5.19 4.88 5.75 4.66 5.12 4.75 5.77 4.95 4.67 5.47 4.92 4.33  BLEU (dev) 25.8 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4  params Ã—106 65  58 60 36 50 80 28 168 53 90  213  Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) Parser Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3 Petrov et al. 
score : 0.5342304049213202


